{"pages":[],"posts":[{"title":"Fiddler的使用及与Charles区别","text":"官方网站： https://www.telerik.com/fiddler 官网下载地址： https://www.telerik.com/download/fiddler 简介：Fiddler是位于客户端和服务器端之间的代理，也是目前最常用的抓包工具之一 。它使用代理地址:127.0.0.1，端口:8888。能够记录客户端和服务器之间的所有请求，可以针对特定的请求，分析请求数据、设置断点、调试web应用、修改请求的数据，甚至可以修改服务器返回的数据，功能非常强大，是web调试的利器。 fiddler抓包原理 配置Web 端配置： 配置浏览器：当启用fiddler为代理时，部分浏览器会提示风险，如Firefox ； 此时可以在浏览器设置里面设置手动代理： 127.0.0.1 端口为 8888， 可以避开风险提示； 可以通过设置解决 ： Firefox： 设置 - 常规 - 网络设置 - 手动代理设置 对于https抓包，可以通过在设置fiddler 解密： Tools -&gt;fiddler options -&gt;HTPPS 中勾选 “Decrypt HTTPS Traffic” 设置信任证书 ： actions : trust_Root_certificate 手机端配置： 设置fiddler 允许远程连接： 手机设置手动代理到fiddler，以IOS 为例： 设置 -&gt;通用 -&gt; 网络 -&gt; wifi wifi设置里面选择手动代理 输入本机ip地址 ， 端口号设为 8888； 对于https抓包，需要手机安装信任证书： 浏览器访问 ：电脑本地IP 地址+ 端口号8888，下载证书 点击下载安装: Fiddler主界面 ： 左侧面板： 主要详细记录每一个请求的详细信息，包含请求结果， Http响应状态，请求地址域名，请求大小，请求响应的类型等。 右侧面板： 依据功能需要，右侧提供了分析详细请求数据，查看具体的请求信息， 以及抓取在线页面进行web 调试，修改服务器返回值等。 常用功能： 查看接口具体的会话内容 应用场景： 需要进行接口定位； 操作步骤： 一般测试时当前端报错，需要观察报错接口时，可以使用fiddler 抓包， 查看具体报错原因； 记录接口的详细请求数据，当需要反复调试接口时，使用这些数据在postman 进行调试，快速定位问题是否已经被解决； 模拟慢网速的情况 应用场景：有时候因为网络波动导致一些异常报错， 需要复现 操作步骤： Fiddler 限速通过延迟发送或接收数据的时间来限制网络的下载和上传速度，从而限速。 使用rules -&gt; performance-&gt; Simulate Modem Speeds 限速时的情况（以下是http://www.speedtest.cn/ 网站测速） 采用默认的不一定符合要求，可以采用自定义配置限速参数：右侧监控面板“FiddlerScript”选项卡 -&gt; find 中输入“m_Simulate” 修改其中相应的请求，响应延迟，单位ms. 不限速时的情况： PS : Fiddler 模拟慢网速的情况相比较于Charles 来说，Charles 效果更显著些，当fiddler 效果不明显时建议使用Charles Charles模拟慢网速使用方法： proxy -&gt; Throtting setting 中： mock数据： 应用场景：需要测试一些异常的case，需要更改任意请求参数 或者响应参数时， 比如前端做了限制不允许输入 操作步骤： 选中我们需要mock的会话 -&gt; 保存服务器返回body内容在任意文件夹 -&gt; 修改body内容， 在 AutoResponder 中导入 -&gt; 勾选 enable rules ,再次请求 设置断点,快速修改返回参数 ： 应用场景：需要拦截所有的请求， 比上述mock更快捷，但仅适用于一次性请求 操作步骤：1. Rules -&gt; Automatic Breakpoint -&gt;before Request 或者 直接点击会话列表下方的断点按钮； \\2. 修改请求参数后，点击 “break to response “查看响应参数 ； \\3. 点击”run to completion“ 查看web端调试结果 重复请求会话 应用场景：网络中断或者需要进行简单的性能测试，频繁调用同一接口 操作步骤：右击选中的请求 -》 replay -》Reissue sequentially -》设置一次请求的次数 PS： 对比Charles， Charles也可以设置重复请求： Fiddler 抓取不到包的原因： 一般来说，打开fiddler 就是抓包状态，如果没有开始抓包见下； 首先确定一下 file-&gt;capture traffic 这个是否勾选—》即页面左下角capturing 是否打开； 如果访问的网站是https网站， 则可以通过Tools - Fiddler Options - Https - Decrypt Http Traffic来设置将https解密； 依然抓不到包，查看是否安装了正确的https证书 手机客户端请求抓不到的原因（这个Charles 与之基本类似）： 是否在同一局域网内； 是否有防火墙拦截； Fiddler(charles) 是否允许远程客户端连接； 监听端口号是否是默认的8888； IOS 手机设置信任证书： 设置 - 通用 - 关于本机 - 证书信任设置 中手动设置信任证书； Fiddler 和Charles: 共同点：抓包，断点调试，请求替换，代理功能 不同点：Charles是收费的，但可以在各大主流平台都可以使用； Fiddler开源免费， 但只能在windows上运行；","link":"/2019/12/21/Fiddler%20vs%20charles/"},{"title":"Ubuntu升级python及设置默认版本","text":"ubuntu 升级 python 2.7 - python 3.6 查询一般办法： 添加安装源ppa： sudo add-apt-repository ppa:jonathonf/python-3.6 加入python3.6 安装源进入系统后，更新软件列表： sudo apt-get update 更新安装源后，安装python3.6： sudo apt-get install python3.6 ​ 报错之后可以采用： sudo add-apt-repository ppa:deadsnakes/ppa 继续执行： sudo apt-get update sudo apt-get install python3.6 依然不成功的话： 可以查看是否系统配置有问题， 或者因为网络原因导致更新源没有更新成功，切换网络试试。 安装完成之后， 接下来设置python3.6的优先级和默认： 调整python3 的优先级： sudo update-alternatives –install /usr/bin/python3 python3 /usr/bin/python3.5 1 sudo update-alternatives –install /usr/bin/python3 python3 /usr/bin/python3.6 2 注： –install 选项使用多个参数用于创建符号链接，最后一个参数指定了此选项的优先级，优先级高的选项会被选中。 此例子中， 因为python3 /usr/bin/python3.6 设置的优先级为2， 所以在update-alternatives命令会在 /usr/bin/python3 版本下自动将python3.6 设置为 python3 默认版本。 设置默认版本 更改python默认，系统默认为python 2，现更改为python3： sudo update-alternatives –install /usr/bin/python python /usr/bin/python2 100 sudo update-alternatives –install /usr/bin/python python /usr/bin/python3 150 注： 此例设置python 2 和 python 3 的优先级，这里python 3 在/usr/bin/python 中优先级更高，所以默认python3 系统切换默认版本： ~$ update-alternatives –list python # 列出所有可用的python替代版本信息 /user/bin/python2 /user/bin/python2 ~$ python –version #查看默认python版本 python 3.6.10 切换默认版本到python2 ~$ sudo update-alternatives –config python lvxiran@lvxiran-virtual-machine:~$ sudo update-alternatives --config python [sudo] lvxiran 的密码： 有 2 个候选项可用于替换 python (提供 /usr/bin/python)。 选择 路径 优先级 状态 ------------------------------------------------------------ * 0 /usr/bin/python3 150 自动模式 1 /usr/bin/python2 100 手动模式 2 /usr/bin/python3 150 手动模式 要维持当前值[*]请按&lt;回车键&gt;，或者键入选择的编号：1 update-alternatives: 使用 /usr/bin/python2 来在手动模式中提供 /usr/bin/python (python) 再次查看python 默认版本已经变成 python2 了 更优的解决办法： ##有关virtualenv 详见 《virtuale环境和flask安装》 安装virtualenv ,实现不同版本之间的切换 mkdir venvflask #新建一个项目文件夹 cd venvflask #进入到该文件夹下 已经安装pip： pip install virtualenv 未安装pip： sudo apt-get install python-virtualenv 创建一个虚拟环境： virtualenv venv 创建一个python包之间没有关联的虚拟环境： virtualenv –no-site-packages venv 激活虚拟环境： source venv/bin/activate 切换Python版本工作时： cd ~/path/to/project #进入项目文件夹 source ~/venvflask/venv/bin/activate #切换到虚拟环境中python版本工作","link":"/2020/02/08/Uuntu%E5%8D%87%E7%BA%A7python%E7%89%88%E6%9C%AC/"},{"title":"批量将Charles抓包文件导入Postman","text":"背景​ 测试小伙伴，经常需要使用postman进行接口测试，使用Charles进行抓包，但如果将Charles抓包内容复制到postman既费时又费力。 ​ 该工具可以帮助批量将Charles导出的抓包请求响应数据文件，快速转成支持导入postman格式。 ​ 该工具是对GitHub 上一个小工具的补充，github项目地址： https://github.com/liyinchigithub/Charles2Postman 环境charles 版本 4.5.6postman 版本 7.26.0node 版本 v11.10.0(大于v8.11.4)python 2.7.16 操作步骤： 将抓包内容从Charles 中导出， 导出格式保存为：JSON session File（.chlsj），保存到batchFile文件夹中右击 -&gt; 导出export seesion , 路径 ./batchFile -&gt; JSON session File（.chlsj） 开始转换， 执行 sh run.sh 输出文件，生成在outputFile文件夹下 ./outputFile/postman_collection.json 将转换文件导入到postman中 ​ 注： 转换过程中，File文件夹中不允许存在重名文件 实现方式 https://github.com/liyinchigithub/Charles2Postman 工具实现了单个接口的转换； 我们使用python对文件进行处理，实现批量转换 # 遍历batchFile 文件夹下的文件 def listDir(): files = os.listdir(path); for oneFile in files: operateFile(oneFile) # 解析batchFile 文件夹下的单个文件，将会话转化为单个可执行node index.js的文件 def operateFile(oneFile): print(type(oneFile)) fullPath = path + oneFile fp = open(fullPath, 'r') json_data = json.load(fp) for i, oneJson in enumerate(json_data): list = [] fullPath = targetPath + 'test' + str(i) + '.chlsj' new_file = open(fullPath, 'w') list.append(oneJson) new_file.write(json.dumps(list)) new_file.close() print('这是文件中的json数据：', type(json_data))","link":"/2020/06/01/charles%20%E6%96%87%E4%BB%B6%E5%AF%BC%E5%85%A5Postman/"},{"title":"Flume 简介","text":"Flume介绍[TOC] 简介Flume是Cloudera 提供的一个高可用的，高可靠的，分布式的海量日志采集，聚合和传输的系统。Flume支持在日志系统中定制各类数据发送方，用于收集数据；同时，Flume提供对数据进行简单处理并写到数据接受方的能力。 简单来说，Flume是一个数据采集工具；可以从各类数据源上采集数据传输到大数据生态的各种存储系统中（Hdfs、hbase、hive、kafka） flume官网： http://flume.apache.org/ 特点 flume 重要组成：source，channel，sink，适用于日志文件实时采集 Flume特点： 分布式：flume分布式集群部署，扩展性好 可靠性好：当节点出现故障时，日志能够被传送到其他节点上而不会丢失 易用性：flume配置使用较繁琐，对使用人员专业技术要求高 实时采集：flume采集流模式进行数据实时采集 Flume使用场景：适用于日志文件实时采集 Flume体系结构运行机制source -&gt; channel -&gt; sink Flume中最核心的角色是agent，flume采集系统就是由一个个agent连接起来所形成的一个或简单或复杂的数据传输通道。 每一个agent（一个独立的守护进程JVM）相当于一个数据传递员，内部有三个组件： Source：数据源组件，用于跟数据源对接，以获取数据；它有各种各样的内置实现； Sink： 下沉组件（输出），用于往下一级agent传递数据或者向最终存储系统传递数据 Channel：缓存通道组件，用于从source将数据传递到sink 相关组件 source源组件：不同数据源，有不同的source 实现类： 如，接受网络消息的source： TCP等； 读取日志文件的source： spooldir, exec (tail -f) 等; 读取kafka中消息的source： kafkasource等. channel 缓存通道：多种实现方式：缓存在内存中， 缓存在本地磁盘文件中 ，缓存在kafka集群中… sink下沉组件：根据目标种类的不同，有不同的具体实现： 写入kafka, 写入hdfs, 写入hbase… 高途大数据侧配置： ​ source： tail -f 监控日志文件变动，读取日志文件的source, 通过 put 操作将数据放到channel中 ​ channel： File channel ， 将数据缓存在本地磁盘文件中 ​ Kafka sink: kafka sink 通过 take操作从channel中取出数据 ​ 应用场景下图取自：https://zhuanlan.zhihu.com/p/346979122（参考文章）","link":"/2021/03/12/flume/"},{"title":"Python keyword 之 with","text":"with: 基本语法格式： with exp [as target]; target 可以是变量或者元组，存储exp返回的结果； With是一种上下文管理协议，其表达式其实是try - finally 的简写形式，但没有except 功能； 使用with处理的对象必须包含方法 enter()和exit()； enter()在方法体执行前运行，初始化代码块，并将返回值赋值给as关键字后的变量; exit() 在方法体执行结束后运行，关闭代码块，释放资源，如文件使用后关闭等。即便代码块抛出异常也调用exit() 退出代码块。 工作原理代码示意 #encoding = utf - 8 class Mytest(): def __init__(self,file_name): self.file_name = file_name def __enter__(self): print(&quot;进入__enter__&quot;) self.f_name = open(self.file_name) return self.f_name def __exit__(self, exc_type, exc_val, exc_tb): print(&quot;进入__exit__&quot;) self.f_name.close() with Mytest(&quot;1.txt&quot;) as f: for line in f.readlines(): print(line.strip()) 输出结果： 进入__enter__ YYYYYYYYYYYYYYYYYY LLLLLLLLLLLL MMMMMMMM 进入__exit__ 将上述代码中执行代码块更改一下： try: with Mytest(&quot;1.txt&quot;) as f: context = f.readlines() print('------') except FileNotFoundError: print(&quot;该文件不存在&quot;) else: for line in context: print(line.strip()) 输出结果： 进入__enter__ ------ 进入__exit__ YYYYYYYYYYYYYYYYYY LLLLLLLLLLLL MMMMMMMM Process finished with exit code 0","link":"/2019/10/19/python_with/"},{"title":"日常小记《virtual环境和flask安装》","text":"virtualenv : 解决需要切换多版本python运行的问题. virtualenv就是用来为一个应用创建一套“隔离”的Python运行环境. 首先： $ pip3 install virtualenv 然后 第一步，创建目录 Mac:~ michael$ mkdir myproject Mac:~ michael$ cd myproject/ Mac:myproject michael$ 第二步，创建一个独立的python运行环境 virtualenv envname Mac:myproject michael$ virtualenv –no-site-packages venv 新建的python 环境会被放到venv目录下， 用source 进入该环境 (linux) Mac:myproject michael$ source venv/bin/activate (windows) venv\\Scripts\\activate.bat 在venv环境下，用pip安装的包都被安装到这个环境； pip install flask 退出当前的venv环境， 用deactivate (linux) (venv)Mac:myproject michael$ deactivate (windows) venv\\Scripts\\deactivate.bat 检测安装版本： pip3 freeze (venv) lvxiran@lvxiran-virtual-machine:~/venvflask$ pip3 freeze Click==7.0 Flask==1.1.1 itsdangerous==1.1.0 Jinja2==2.11.1 MarkupSafe==1.1.1 Werkzeug==1.0.0","link":"/2020/02/05/virtual_flask/"},{"title":"《selenium 基本原理》","text":"Selenium 的简介： Selenium1.0 ： Selenium core = Selenium Remote Control (RC )+ Selenium IDE (集成开发环境） Selenium 2.0 : = Selenium1 + WebDriver Webdriver 用来绕过JavaScript环境的沙盒限制， 可以直接让测试工具调用浏览器和操作系统本身提供的内置方法。 Selenium 官网地址： www.seleniumhq.org Selenium RC 的实现原理 selenium1的自动化执行步骤： 测试人员基于Selenium支持的编程语言编写好测试脚本程序。 测试人员执行测试程序。 测试脚本程序发送访问网站的HTTP请求给Remote Control Server（RC）。 RC 收到请求后，访问被测试网站并获取网页数据内容，并在网页中插入Selenium Core的JavaScript代码库，然后返回给测试人员执行测试的浏览器。 测试脚本在浏览器内部再调用selenium Core来执行测试代码逻辑，记录测试结果，完成测试 理解以上步骤： 理解“ 同源策略” —-浏览器的JavaScript 安全机制 相同的协议，端口，域名 Selenium 1工具的核心部分是基于JavaScript代码库来实现的，这个库默认地和被测网站分离，它的URL和被测网站的域名是不一致的，为了绕过浏览器安全机制，Selenium 1.0 作者使用代理方式解决问题 selenium1代理模式的实现机制： 执行测试脚本，脚本向Selenium Server发起请求，要求和Selenium Server建立链接。 Selenium Server的Launcher启动浏览器，向浏览器中插入Selenium Core的JavaScript代码库，并把浏览器的代理设置为Selenium Server的Http Proxy，确保后续Core的脚本域被访问的网站的脚本同源。 测试脚本向seleniumServer发送Http请求，Selenium Server对请求进行解析，然后通过Http Proxy发送JS命令通知Selenium Core发送JS命令通知Seleniim Core执行操作浏览器的请求。 Selenium Core收到指令后，执行测试脚本里指定的网页操作命令。 浏览器收到新的请求信息，于是发送Http请求给Selenium给Selenium Core里的Http Proxy，请求新的Web页面。（因为第二步中，selenium Server在启动浏览器的时候，已经把浏览器的代理地址设定为Selenium Server的Http Proxy） Selenium Server接收到请求后，自行重组http请求，向应用服务器发送请求并获取返回的web页面。 Selenium Server的Http Proxy把接收到的Web页面返回给浏览器。 WebDriver 的实现原理 与 selenium 1注入不同，直接利用浏览器的内部接口来操作浏览器； 优点： 基于浏览器内部接口实现自动化测试，接近用户使用的真实情况； selenium必须操作浏览器，webdriver 可以使用HTMLunit进行测试，不打开浏览器进行快速测试 提供了简介的面向对象API ， 提高测试脚本的编写效率 使用过程中无须单独启动selenium server","link":"/2019/11/17/selenium/selenium01/"},{"title":"《Selenium IDE》","text":"Selenium IDE： Selenium IDE仅作为Mozilla Firefox和Chrome插件提供，可实现网页操作步骤的录制和回放， 执行简单测试逻辑的自动化测试， 可以将录制的测试脚本导出为C#，Java，Ruby或Python等编程语言 主界面如图： 常用工具栏**： 从左到右依次是： 执行所有case按钮， 执行当前case按钮，单步执行按钮，测试用例执行速度控制栏， 断点， 暂停录制，录制脚本按钮 Selenium IDE脚本介绍： Commond 命令也成为selenese; Selenese 最多有两个参数（target 和 value ),也可以有一个（target） 或者没有参数； Selenese 命令类型： Actions 类型： 如click 命令表示单击页面元素； type命令会在页面文本框输入文字，输入的内容显示在文本框中 Accessors 存储器类型： 如storeTitle存储型命令，将页面的title信息读取出来，并存储到变量中，与页面元素本身没有交互 Assertions断言类型： 如 Assert， Verify，waitFor 部分验证点操作： 选中一个验证点， 编辑脚本中可以更改验证点： “右键” 插入一个新的command， 查看编辑脚本区，列表展示合适的验证点 以上录制脚本中： case全部通过， 左下角显示绿色进度条说明case通过； 如果没有通过，则显示红色提示，说明失败case; 已经跑完通过的case在脚本显示区为绿色 未执行完成的case为黄色 尚未执行的case为灰色 执行失败的case为红色 tips： Selenium IDE：学习此插件需要熟悉HTML, JavaScript, DOM Selenium IDE 仅适用于执行简单逻辑的自动化测试脚本，不适用大项目","link":"/2019/11/18/selenium/selenium02/"},{"title":"初识WebDriver","text":"WebDriver原理： Selenium Webdriver是通过各种浏览器的驱动（web driver）来驱动浏览器，原理示意图如下： 在python中安装WebDriver： 完成python，pip工具的安装后，在cmd下输入“pip install selenium”， 完成安装后输入”python” 进入python 交互模式， 输入“import selenium” 没有报错即成功安装selenium。 安装浏览器驱动： Firefox 浏览器的驱动： https://github.com/mozilla/geckodriver/releases Google浏览器驱动： http://npm.taobao.org/mirrors/chromedriver/ 选择适合自己操作系统的版本进行下载。 第一个webDriver 脚本： #-*-coding:UTF-8-*- fromseleniumimportwebdriver fromselenium.common.exceptionsimportNoSuchElementException importtime importselenium.webdriver.support.uiasui #通过executable_path参数指明Firefox驱动文件所在路径 #路径查询：windows系统或者采用C:/Users/34954/Downloads/geckodriver driver=webdriver.Firefox(executable_path=&quot;C:\\\\Users\\\\34954\\\\Downloads\\\\geckodriver&quot;) #若定位元素时间较长，可用10s内每隔500ms扫描1次页面变化，当出现指定元素后结束 wait=ui.WebDriverWait(driver,10) #打开百度首页 driver.get(&quot;https://baidu.com&quot;) try: wait.until(lambdadriver:driver.find_element_by_id(&quot;kw&quot;)) #清空搜索输入框默认内容 driver.find_element_by_id(&quot;kw&quot;).clear() #在搜索输入框内输入“自动化测试” driver.find_element_by_id(&quot;kw&quot;).send_keys(&quot;自动化测试&quot;) #单击“搜索”按钮 driver.find_element_by_id(&quot;su&quot;).click() #若定位不到元素，抛出异常 exceptNoSuchElementExceptionasmsg: print(&quot;查找元素异常&quot;) #等待3秒 time.sleep(3) #退出浏览器 driver.quit() 4.1 问题及建议 问题 1. 报错提示NoSuchElementException: Unable to locate element 解决办法： 去网页相应的元素下查找相关的element_id， 如url为sogou.com时对应的查询id为“query”，对应的搜索button id为“stb”","link":"/2019/11/24/selenium/selenium03/"},{"title":"Kafka 简介","text":"kafka简介[TOC] 官网：https://kafka.apachecn.org/ 背景 ​ Kafka是最初由Linkedin公司开发，是一个分布式、支持分区的（partition）、多副本的（replica），基于zookeeper协调的分布式发布订阅消息系统，它的最大的特性就是可以实时的处理大量数据以满足各种需求场景：比如基于hadoop的批处理系统、低延迟的实时系统、storm/Spark流式处理引擎，web/nginx日志、访问日志，消息服务等等，用scala语言编写，Linkedin于2010年贡献给了Apache基金会并成为顶级开源 项目。 Kafka 个高吞吐量、分布式的发布一订阅消息系统。据 Kafka 官方网站介绍，当前的Kafka 己经定位为 个分布式流式处理平台（ distributed earning platform ），它最初由 Linkedin公司开发，后来成为 Apache 项目的一部分。 Kafka 核心模块使用 Scala 语言开发，支持多语言（如 Java IC＋＋ rthon Go Erlang Node.js 等）客户端，它以可水平扩展和具有高吞吐量等特性而被广泛使用 目前越来越多的开源分布式处理系统（如 Flume Apache Storm Spark Flink等）支持与 Kafka 集成。 Kafka基本特性： 高吞吐量、低延迟：kafka每秒可以处理几十万条消息，它的延迟最低只有几毫秒 可扩展性：kafka集群支持热扩展 持久性、可靠性：消息被持久化到本地磁盘，并且支持数据备份防止数据丢失 容错性：允许集群中节点失败（若副本数量为n,则允许n-1个节点失败） 高并发：支持数千个客户端同时读写 基本架构 基本概念 Producer：Producer即生产者，消息的产生者，是消息的入口。 Broker：Broker是kafka实例，每个服务器上有一个或多个kafka的实例，我们姑且认为每个broker对应一台服务器。每个kafka集群内的broker都有一个不重复的编号，如图中的broker-0、broker-1等…… Topic：消息的主题，可以理解为消息的分类，kafka的数据就保存在topic。在每个broker上都可以创建多个topic。 Partition：Topic的分区，每个topic可以有多个分区，分区的作用是做负载，提高kafka的吞吐量。同一个topic在不同的分区的数据是不重复的，partition的表现形式就是一个一个的文件夹！ Replication: 每一个分区都有多个副本，副本的作用是做备胎。当主分区（Leader）故障的时候会选择一个备胎（Follower）上位，成为Leader。在kafka中默认副本的最大数量是10个，且副本的数量不能大于Broker的数量，follower和leader绝对是在不同的机器，同一机器对同一个分区也只可能存放一个副本（包括自己）。 Message：每一条发送的消息主体。 Consumer：从Kafka集群中消费消息的终端或服务。 Consumer Group：我们可以将多个消费组组成一个消费者组，在kafka的设计中同一个分区的数据只能被consumer group 中的某一个consumer 消费。同一个consumer group的consumer可以消费同一个topic的不同分区的数据，这也是为了提高kafka的吞吐量！ Zookeeper：kafka集群依赖zookeeper来保存集群的的元信息，来保证系统的可用性。 Kafka应用场景 消息系统：发布-订阅，解耦生产者和消费者，缓存消息等 存储系统：充当中间数据的存储系统，是一种高性能、低延迟、具备日志存储、备份和传播功能的分布式文件系统。 日志收集：一个公司可以用Kafka可以收集各种服务的log，通过kafka以统一接口服务的方式开放给各种consumer，例如hadoop、Hbase、Solr等 运营指标：用来记录运营监控数据。包括收集各种分布式应用的数据，生产各种操作的集中反馈，比如报警和报告。 用户活动跟踪：Kafka经常被用来记录web用户或者app用户的各种活动，如浏览网页、搜索、点击等活动，这些活动信息被各个服务器发布到kafka的topic中，然后订阅者通过订阅这些topic来做实时的监控分析，或者装载到hadoop、数据仓库中做离线分析和挖掘。 流处理：比如spark streaming, Strom和 Flink 安装Kafka1. 安装Windows系统下安装 ： https://www.jianshu.com/p/ce203d4e2f41 2. 启动2.1 启动kafka之前需要先启动 zookeeper zookeeper 和 Kafka 有几种不同的启动方式 zookeeper启动： 方式一：brew services start zookeeper 方式二： zkServer start kafka启动： 方式一：brew services start kafka 方式二：zookeeper-server-start /usr/local/etc/kafka/zookeeper.properties &amp; kafka-server-start /usr/local/etc/kafka/server.properties 方式一是后台运行，方式二是常用的启动方式。 2.2 查看是否启动成功 ps aux | grep kafka 3. 应用3.1 创建topic kafka-topics –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic topic_a 查看已有主题list kafka-topics –list –zookeeper localhost:2181 3.2 创建生产者 kafka-console-producer –topic topic_a –broker-list localhost:9092 3.3 创建消费者 kafka-console-consumer –bootstrap-server localhost:9092 -topic topic_a Kafka&amp;flume 高途业务实时数据 前端通过SDK 或者后端埋点接口上报的方式，将用户行为数据上传到nignx服务器，并落地服务器本地磁盘上，flume收集上报的数据，通过消息队列（kafka,MQ ,databus) 推给flink任务。 后端日志也可以直接生产消息到kafka，然后将消息内容推给flink任务。 数据采集及落地 ​ 直播端/内容端/APP等通过sdk或后端埋点的方式进行数据上报，flume收集数据上报的log，通过消息队列推给flink任务，flink任务对消息进行过滤，过滤出要进行处理的的有效消息，进行相应的数据处理统计落库，分别写入到redis和hbase，hbase的结果数据会通过mr最终存储到HDFS。同时将有效的内容再反写入kafka，通过kafka-loader将这些数据埋点数据存储在HDFS。而在flume将埋点数据推到kafka的时候也会将全量的埋点的数据load到HDFS中。 ​ 埋点上报拆分 ​ 用户行为数据通过消息队列（kafka）推动到flink任务， flink任务对消息进行过滤， 将过滤的有效内容反写入Kafka中，即上述有效的Kafka topic ，最终通过kafka-loader将这些有效数据分别存储在对应的Hbase中。 大数据侧Kafka应用： databus flume - 数据采集 flink - 数据处理 测试常见问题 数据丢失： 1. ACK （0，-1，1）之间消息传递； 2. leader &amp; follower 之间数据同步问题； 3. C端消息push， offset设置问题 消息堆积导致消息延迟 高峰期重启kafka集群，可能存在数据丢失； 因为Kafka扩容或者Kafka集群抖动 -&gt; flink任务失败 kafka消息丢失及解决办法： https://juejin.cn/post/6844904094021189639 kafka消息重复和丢失的场景及解决办法： https://www.cnblogs.com/wangzhuxing/p/10124308.html 参考文章：《Kafka基本原理详解》 https://my.oschina.net/u/4377703/blog/4325442 ​ 官网：https://kafka.apachecn.org/intro.html","link":"/2020/11/27/Kafka/Kafka/"},{"title":"Kafka安装与使用","text":"MAC： 安装 brew install kafka 踩坑： 第一次安装失败，由于公司网有限速，故下载过程中了解超时，某些依赖下载失败； 故连接个人热点，重新下载安装，如上图安装成功。 通过下载依赖包一步步安装方式参考文章： https://blog.csdn.net/lisongyue123/article/details/107830976 启动 2.1 启动kafka之前需要先启动 zookeeper 如图一， zookeeper 和 Kafka 有几种不同的启动方式 zookeeper启动： 方式一：brew services start zookeeper 方式二： zkServer start kafka启动： 方式一：brew services start kafka 方式二：zookeeper-server-start /usr/local/etc/kafka/zookeeper.properties &amp; kafka-server-start /usr/local/etc/kafka/server.properties 方式一是后台运行，方式二是常用的启动方式。 2.2 查看是否启动成功 ps aux | grep kafka 应用 3.1 创建topic kafka-topics –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic topic_a 查看已有主题list kafka-topics –list –zookeeper localhost:2181 3.2 创建生产者 kafka-console-producer –topic topic_a –broker-list localhost:9092 3.3 创建消费者 kafka-console-consumer –bootstrap-server localhost:9092 -topic topic_a Linux 安装 ：http://kafka.apache.org/downloads.html 官网下载或者使用命令的方式安装： sudo wget https://mirrors.tuna.tsinghua.edu.cn/apache/kafka/2.6.0/kafka_2.13-2.6.0.tgz ​ 如果下载网速过慢，下面链接总有一个适合你☞ ​ https://mirror.bit.edu.cn/apache/kafka/2.6.0/kafka_2.13-2.6.0.tgz https://mirrors.bfsu.edu.cn/apache/kafka/2.6.0/kafka_2.13-2.6.0.tgz https://mirrors.tuna.tsinghua.edu.cn/apache/kafka/2.6.0/kafka_2.13-2.6.0.tgz https://downloads.apache.org/kafka/2.6.0/kafka_2.13-2.6.0.tgz ​ 踩坑记录： 因下载目录没有操作权限，所以一直提示保存压缩包失败，使用root或者sudo权限就可以。 cd /usr/local/src wget https://mirrors.tuna.tsinghua.edu.cn/apache/kafka/2.6.0/kafka_2.13-2.6.0.tgz tar -zxvf kafka_2.13-2.6.0.tgz mv kafka_2.13-2.6.0 /usr/local/kafka cd /usr/local/kafka 2. 启动 需要先启动zookeeper: bin/zookeeper-server-start.sh config/zookeeper.properties &amp; bin/zookeeper-server-start.sh config/zookeeper.properties &amp; 然后启动kafka: bin/kafka-server-start.sh config/server.properties &amp; 3. 基本应用 创建topic： bin/kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic test 创建启动producer: bin/kafka-console-producer.sh –broker-list localhost:9092 –topic test 创建启动consumer ： bin/kafka-console-consumer.sh –zookeeper 10.202.4.179:2181 –topic test –from-beginning 关闭kafka: bin/kafka-server-stop.sh config/server.properties 4. 其他​ 相关配置介绍： https://blog.csdn.net/suifeng3051/article/details/48053965 ​ https://www.cnblogs.com/toutou/p/linux_install_kafka.html","link":"/2020/11/05/Kafka/MAC%20&%20Linux%E4%B8%8BKafka%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"title":"Kafka脚本之python","text":"Kafka应用Python脚本创建kafka消息 producer.py # -*- coding: utf:-8 -*- # @Time : 2021/2/18 下午4:34 # @Author : lvxiran import sys import json from kafka import KafkaProducer if __name__ == '__main__': topic = 'wenzai_live_data_class_end_test' data = { &quot;behavior&quot;: &quot;start_processing&quot;, &quot;room_id&quot;: &quot;99999&quot;, &quot;timestamp&quot;: &quot;1605509457000&quot;, &quot;room_begin_time&quot;: &quot;1605268800000&quot;, &quot;room_end_time&quot;: &quot;1605509820000&quot;, &quot;lesson_map&quot;: {&quot;15229178744&quot;: &quot;1001&quot;, &quot;10086&quot;: &quot;1002&quot;}, &quot;section_map&quot;: {&quot;2&quot;: &quot;3&quot;, &quot;3&quot;: &quot;4&quot;} } print(topic, data) producer = KafkaProducer(bootstrap_servers='172.0.0.1:9092') producer.send(topic, json.dumps(data).encode('utf-8')) producer.close() Consumer.py #!/usr/bin/env python # -*- coding: utf-8 -*- # @Time : 2021/02/09 # @Author : Lvxiran from kafka import KafkaConsumer topic = 'wenzai_live_data_class_end_test' consumer = KafkaConsumer(topic,group_id='1234', bootstrap_servers=['172.0.0.1:9092']) try: for msg in consumer: print(msg) except KeyboardInterrupt as e: print(e) kafka-python 的基本使用：https://zhuanlan.zhihu.com/p/38330574","link":"/2021/03/03/Kafka/python%20%E8%84%9A%E6%9C%AC%E5%88%9B%E5%BB%BAkafka%E6%B6%88%E6%81%AF/"},{"title":"Jmeter分布式","text":"分布式执行压测压测过程最好使用非-GUI模式执行压测 我们在启动gui模式的时候也会有提示：GUI模式仅用来做调试，压测时用命令行模式 命令行执行：jmeter -n -t /../.jmx -r -l /…jtl -e -o /..file_report_name 解释： jmeter是我们的命令，即apache-jmeter-5.2.1/bin/jmeter -n：表示以命令行形式启动jmeter -t：指定压测脚本.jmx文件 -r：分布式形式启动（提前配置好master机器和slave机器） -l：指定日志文件.jtl文件（该文件存放所有请求记录，文件比较大） -e：在脚本运行结束后生成html报告 -o：指定报告目录 如果在命令行压测中没有生成html报告：可以使用 jmeter -g /…jtl -e -o /..file_report_name 生成html报告 分布式压测配置 Jmeter分布式执行原理： a、Jmeter分布式测试时，选择其中一台作为控制机(master)，其它机器做为代理机(slave)。 b、执行时，master会把脚本发送到每台slave上，salve 拿到脚本后开始执行，salve执行时不需要启动Jmeter，只需要把jmeter-server.bat(linux上是jmeter-server)文件打开，它应该是通过命令行模式来执行的。 c、执行后，slave会把结果回传给master，master会收集所有slave的信息并汇总。 代理机器（slave）配置：a、首先安装好JDK，jmeter，配置好环境变量（版本需要与控制机统一）b、查看本机IP地址c、编辑Jmeter/bin/jmeter.properties，找到”#server_port=1099”,把这一行#去掉，1099是端口号，可以自定义。找到“#server.rmi.ssl.disable=flase”，改为“server.rmi.ssl.disable=true”d、打开jmeter-server.bat文件（linux中打开jmeter-server），就设置完成了，等待控制机(master)启动。 控制机(master)配置：a、master机上需要安装JDK、Jmeter，并且配置好环境变量。b、查看本机IP地址c、打开Jmeter/bin/jmeter.properties，找到”remote_hosts=127.0.0.1”,把这一行修改为”remote_hosts=192.168.8.149:1099,192.168.8.174:1099，1099是端口号，可以随意自定义。如果有多台代理机，这里需要把所有的代理机的IP地址和端口号都加入进来。找到“#server.rmi.ssl.disable=flase”，改为“server.rmi.ssl.disable=true”d、打开jmeter-server.bat文件（linux中打开jmeter-server），设置完成了。 开始添加线程组和请求来运行查看结果：a、打开jmeter文件，添加线程组，编辑线程数，这里设置500个线程数，持续执行5分钟，就是一台机器发送500300(s)个请求。或者设置500个线程数，循环执行2次，就是一台机器发送5002个请求。b、然后添加HTTP请求开始编写脚本。c、添加察看结果数和聚合报告，点击运行，可以选择远程启动或者远程全部启动；如果是点击远程启动，可以选择任意一台电脑来运行；如果是点击远程全部启动就会运行控制机和所有的代理机。d、这里选择远程全部启动，运行结束后，查看聚合报告；如果每台电脑设置的线程数为200，这里一共是两台电脑，所以是200*2=400个线程数。 注意：将配置文件及配置路径，在slave机器上设置好","link":"/2020/07/08/Jmeter/Jmeter%E5%88%86%E5%B8%83%E5%BC%8F/"},{"title":"Jmeter基础应用","text":"Jmeter简介 多线程框架 - 支持多并发操作 用于对服务器模拟负载 支持web, 数据库，FTP服务器系统的性能测试 开源，纯JAVA,可二次定制化开发 mac端下载配置环境 参考文档： https://www.jianshu.com/p/bce9077d883c 配置完成java 环境之后 下载安装jmeter : http://jmeter.apache.org/download_jmeter.cgi 解压之后，可以通过cd 到 jmeter.sh 文件所在目录 ~/user/apacher-jmeter-5.2.1/lib,然后直接 sh jmeter 即可启动 优化： 由于每次访问jmeter 都需要进入到jmeter 的bin 目录下，有点繁琐，所以我们可以将jmeter 配置到环境变量中 vim .bash_profile 在bash_profile 文件中配置path, classpath 保存之后输入 source ~./bash_profile 然后，直接输入jmeter 即可启动jmeter Jmeter脚本编写： 右键测试计划 -&gt; 添加 -&gt;threads -&gt; 线程组 添加http请求： 右击线程组-&gt;添加-&gt;取样器-&gt; http请求 查看请求结果：右击线程组-&gt;添加-&gt;监听器 -&gt;查看结果树，聚合报告，断言结果等 如图聚合报告： Samples/样本： 本次测试中一共发出了多少个请求 Average/平均值： 平均响应时间 Median/响应时间中位数： 即50％ 用户的响应时间 90%Line/90%百分位 ：90％ 用户的响应时间 Min/最小值：最小响应时间 Max/最大值：最大响应时间 Error%/异常：本次测试中出现错误的请求的数量/请求的总数 Throughput/吞吐量: 默认情况下每秒完成的请求数 KB/Sec：每秒从服务器端接收到的数据量 如图查看结果树： 请求绿色表示通过，红色表示失败； 查看取样结果，详细的请求头，请求参数和响应头，响应时间等。 性能测试规范及基础知识： https://help.aliyun.com/document_detail/29337.html?spm=5176.13072777.J_7215278710.1.3c9c26e27nmcPh 监控： 瓶颈分析： 分析 瓶颈定位的目的是对系统中存在的瓶颈点进行分析，为调优做准备，系统的性能瓶颈点主要分布在操作系统资源、中间件参数配置、数据库问题以及应用算法上，对于有针对性的进行调优，有利于系统性能的提升。 风险 当系统的瓶颈点不能被分析出来以后，新业务上线或者核心业务就存在风险，这种风险有可能导致业务高峰的时候，系统性能体验差，甚至“崩溃”。 规范 分析系统的瓶颈点遵循的规则如下： 操作系统资源消耗：CPU、Memory、Disk I/O、Network I/O。 中间件指标：线程池（Thread Pool）、数据库连接池（JDBC）、JVM（GC/FULL GC/堆大小）。 数据库指标：效率低下SQL、锁等待/死锁、缓存命中率、会话、进程等。 应用：方法耗时、算法、同步和异步、缓存、缓冲。 压力机：压力机资源消耗，一般情况下，压力机成为瓶颈的可能性非常低。PTS压力机有保护和调度机制不用单独关","link":"/2020/06/15/Jmeter/Jmeter%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"fiddler","slug":"fiddler","link":"/tags/fiddler/"},{"name":"ubuntu","slug":"ubuntu","link":"/tags/ubuntu/"},{"name":"接口测试","slug":"接口测试","link":"/tags/%E6%8E%A5%E5%8F%A3%E6%B5%8B%E8%AF%95/"},{"name":"Hadoop","slug":"Hadoop","link":"/tags/Hadoop/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"flask","slug":"flask","link":"/tags/flask/"},{"name":"selenium","slug":"selenium","link":"/tags/selenium/"},{"name":"webdriver","slug":"webdriver","link":"/tags/webdriver/"},{"name":"kafka","slug":"kafka","link":"/tags/kafka/"},{"name":"Jmeter","slug":"Jmeter","link":"/tags/Jmeter/"}],"categories":[{"name":"接口测试","slug":"接口测试","link":"/categories/%E6%8E%A5%E5%8F%A3%E6%B5%8B%E8%AF%95/"},{"name":"python","slug":"python","link":"/categories/python/"},{"name":"大数据","slug":"大数据","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"selenium","slug":"selenium","link":"/categories/selenium/"},{"name":"中间件","slug":"中间件","link":"/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"性能测试","slug":"性能测试","link":"/categories/%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"}]}